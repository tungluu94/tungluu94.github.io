papers:
  - name: plare
    title:
      name: 'Policy Learning from Large Vision-Language Model Feedback Without Reward Modeling'
      link: ''
    authors:
      - me: true
      - name: Donghoon Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Speech%20Processing-,Donghoon%20Lee,-%E2%97%8B%20Email%20Address'
      - name: Younghwan Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,YoungHwan%20Lee,-%E2%97%8B%20Email%20Address'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025'
    description: >-
      Offline reinforcement learning (RL) provides a powerful framework for training robotic agents using pre-collected, 
      suboptimal datasets, eliminating the need for costly, time-consuming, and potentially hazardous online interactions. 
      This is particularly useful in safety-critical real-world applications, where data collection is expensive and 
      impractical. However, existing offline RL algorithms typically require reward labeled data, which introduces an 
      additional bottleneck: reward function design is itself costly, labor-intensive, and requires significant domain 
      expertise. In this paper, we introduce PLARE , a novel approach that leverages large vision-language models 
      (VLMs) to provide guidance signals for agent training. Instead of relying on manually designed reward functions, 
      PLARE  queries a VLM for preference labels on pairs of visual trajectory segments based on a language task description. 
      The policy is then trained directly from these preference labels using a supervised contrastive preference learning 
      objective, bypassing the need to learn explicit reward models. Through extensive experiments on robotic manipulation 
      tasks from the MetaWorld, PLARE achieves performance on par with or surpassing existing state-of-the-art VLM-based 
      reward generation methods. Furthermore, we demonstrate the effectiveness of PLARE in real-world manipulation tasks
      with a physical robot, further validating its practical applicability.
#    links:
#      - name: arXiv
#        href: 'https://www.arxiv.org/abs/2506.12822'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: erlvlm
    title:
      name: 'Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models'
      link: 'https://www.arxiv.org/abs/2506.12822'
    authors:
      - me: true
      - name: Younghwan Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,YoungHwan%20Lee,-%E2%97%8B%20Email%20Address'
      - name: Donghoon Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Speech%20Processing-,Donghoon%20Lee,-%E2%97%8B%20Email%20Address'
      - name: Sunho Kim
        href: 'https://sites.google.com/view/kaist-roboticslab/members?authuser=0#:~:text=Han%20%5BCV%5D-,Sunho%20Kim,-%5BCV%5D'
      - name: Min Jun Kim
        href: 'https://ee.kaist.ac.kr/en/professor/17499/'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'International Conference on Machine Learning (ICML), 2025'
    description: >-
      Designing effective reward functions remains a fundamental challenge in reinforcement learning (RL), as it often 
      requires extensive human effort and domain expertise. While RL from human feedback has been successful in 
      aligning agents with human intent, acquiring high-quality feedback is costly and labor-intensive, limiting 
      its scalability. Recent advancements in foundation models present a promising alternative--leveraging 
      AI-generated feedback to reduce reliance on human supervision in reward learning. Building on this paradigm, 
      we introduce ERL-VLM, an enhanced rating-based RL method that effectively learns reward functions from AI 
      feedback. Unlike prior methods that rely on pairwise comparisons, ERL-VLM queries large vision-language models 
      (VLMs) for absolute ratings of individual trajectories, enabling more expressive feedback and improved sample 
      efficiency. Additionally, we propose key enhancements to rating-based RL, addressing instability issues caused 
      by data imbalance and noisy labels. Through extensive experiments across both low-level and high-level control 
      tasks, we demonstrate that ERL-VLM significantly outperforms existing VLM-based reward generation methods.
      Our results demonstrate the potential of AI feedback for scaling RL with minimal human intervention, paving 
      the way for more autonomous and efficient reward learning.
    links:
      - name: arXiv
        href: 'https://www.arxiv.org/abs/2506.12822'
      - name: openreview
        href: 'https://openreview.net/forum?id=k77bq8AJVy'
      - name: website
        href: 'https://erlvlm2025.github.io/'
      - name: code
        href: 'https://github.com/tunglm2203/erlvlm'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: lvlm2p
    title:
      name: 'Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation'
      link: 'https://arxiv.org/abs/2505.11221'
    authors:
      - name: Donghoon Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Speech%20Processing-,Donghoon%20Lee,-%E2%97%8B%20Email%20Address'
        star: true
      - me: true
        star: true
      - name: Younghwan Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,YoungHwan%20Lee,-%E2%97%8B%20Email%20Address'
        star: false
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025'
    description: >-
      Recent research highlights the potential of multi-modal foundation models in tackling complex decision-making 
      challenges. However, their large parameters make real-world deployment resource-intensive and often impractical 
      for constrained systems. Reinforcement learning (RL) shows promise for task-specific agents but suffers from 
      high sample complexity, limiting practical applications. To address these challenges, we introduce LVLM to 
      Policy (LVLM2P), a novel framework that distills knowledge from large vision-language models (LVLM) into more 
      efficient RL agents. Our approach leverages the LVLM as a teacher, providing instructional actions based on 
      trajectories collected by the RL agent, which helps reduce less meaningful exploration in the early stages of 
      learning, thereby significantly accelerating the agent’s learning progress. Additionally, by leveraging the LVLM 
      to suggest actions directly from visual observations, we eliminate the need for manual textual descriptors of the
      environment, enhancing applicability across diverse tasks. Experiments show that LVLM2P significantly enhances 
      the sample efficiency of baseline RL algorithms.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2505.11221'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10888998'
      - name: code
        href: 'https://github.com/i22024/LVLM2P'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: rgvlm
    title:
      name: 'Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning'
      link: 'https://arxiv.org/abs/2504.08772'
    authors:
      - name: Younghwan Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,YoungHwan%20Lee,-%E2%97%8B%20Email%20Address'
        star: True
      - me: true
        star: True
      - name: Donghoon Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Speech%20Processing-,Donghoon%20Lee,-%E2%97%8B%20Email%20Address'
        star: false
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025 (<oral>Oral</oral>)'
    description: >-
      In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains 
      where real-time interaction with the environment is expensive or risky. However, designing dense reward signals 
      for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human 
      feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, 
      prompting interest in automated reward generation models. To address this, we propose Reward Generation via 
      Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards 
      from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be 
      seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential 
      as an auxiliary reward signal.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2504.08772'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10889042'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: pcdt
    title:
      name: 'Predictive Coding for Decision Transformer'
      link: 'https://arxiv.org/abs/2410.03408'
    authors:
      - me: true
        star: True
      - name: Donghoon Lee
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Speech%20Processing-,Donghoon%20Lee,-%E2%97%8B%20Email%20Address'
        star: True
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024'
    description: >-
      Recent work in offline reinforcement learning (RL) has demonstrated the effectiveness of formulating 
      decision-making as return-conditioned supervised learning. Notably, the decision transformer (DT) architecture 
      has shown promise across various domains. However, despite its initial success, DTs have underperformed on 
      several challenging datasets in goal-conditioned RL. This limitation stems from the inefficiency of return 
      conditioning for guiding policy learning, particularly in unstructured and suboptimal datasets, resulting in 
      DTs failing to effectively learn temporal compositionality. Moreover, this problem might be further exacerbated 
      in long-horizon sparse-reward tasks. To address this challenge, we propose the Predictive Coding for 
      Decision Transformer (PCDT) framework, which leverages generalized future conditioning to enhance DT methods. 
      PCDT utilizes an architecture that extends the DT framework, conditioned on predictive codings, enabling 
      decision-making based on both past and future factors, thereby improving generalization. Through extensive 
      experiments on eight datasets from the AntMaze and FrankaKitchen environments, our proposed method achieves 
      performance on par with or surpassing existing popular value-based and transformer-based methods in offline 
      goal-conditioned RL. Furthermore, we also evaluate our method on a goal-reaching task with a physical robot.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2410.03408'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10802437'
      - name: code
        href: 'https://github.com/tunglm2203/pcdt'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: vq_robust_rl
    title:
      name: 'Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization'
      link: 'https://arxiv.org/abs/2410.03376'
    authors:
      - me: true
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - name: Tee Joshua Tian Jin
      - name: Sungwoong Kim
        href: 'https://sites.google.com/view/swkim'
        star2: false
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star2: false
    conference:
      name: 'IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024'
    description: >-
      Recent studies reveal that well-performing reinforcement learning (RL) agents in training often lack resilience 
      against adversarial perturbations during deployment. This highlights the importance of building a robust agent 
      before deploying it in the real world. Most prior works focus on developing robust training-based procedures to 
      tackle this problem, including enhancing the robustness of the deep neural network component itself or 
      adversarially training the agent on strong attacks. In this work, we instead study an input transformation-based 
      defense for RL. Specifically, we propose using a variant of vector quantization (VQ) as a transformation 
      for input observations, which is then used to reduce the space of adversarial attacks during testing, resulting 
      in the transformed observations being less affected by attacks. Our method is computationally efficient and 
      seamlessly integrates with adversarial training, further enhancing the robustness of RL agents against adversarial 
      attacks. Through extensive experiments in multiple environments, we demonstrate that using VQ as the input 
      transformation effectively defends against adversarial attacks on the agent's observations.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2410.03376'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10802066'
      - name: code
        href: 'https://github.com/tunglm2203/vq_robust_rl'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: scalable_softgroup
    title:
      name: 'Scalable SoftGroup for 3D Instance Segmentation on Point Clouds'
      link: 'https://arxiv.org/abs/2209.08263'
    authors:
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Kookhoi Kim
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - me: true
      - name:  Junyeong Kim
        href: 'https://sites.google.com/view/imr-lab'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024'
    description: >-
      This paper considers a network referred to as SoftGroup for accurate and scalable 3D instance segmentation. 
      Existing state-of-the-art methods produce hard semantic predictions followed by grouping instance segmentation 
      results. Unfortunately, errors stemming from hard decisions propagate into the grouping, resulting in poor 
      overlap between predicted instances and ground truth and substantial false positives. To address the abovementioned 
      problems, SoftGroup allows each point to be associated with multiple classes to mitigate the uncertainty 
      stemming from semantic prediction. It also suppresses false positive instances by learning to categorize 
      them as background. Regarding scalability, the existing fast methods require computational time on the order 
      of tens of seconds on large-scale scenes, which is unsatisfactory and far from applicable for real-time. Our 
      finding is that the k-Nearest Neighbor (k-NN) module, which serves as the prerequisite of grouping, introduces 
      a computational bottleneck. SoftGroup is extended to resolve this computational bottleneck, referred to as 
      SoftGroup++. The proposed SoftGroup++ reduces time complexity with octree k-NN and reduces search space with 
      class-aware pyramid scaling and late devoxelization. Experimental results on various indoor and outdoor 
      datasets demonstrate the efficacy and generality of the proposed SoftGroup and SoftGroup++. Their performances 
      surpass the best-performing baseline by a large margin (6% ∼ 16%) in terms of AP50. On datasets with large-scale 
      scenes, SoftGroup++ achieves a 6× speed boost on average compared to SoftGroup. Furthermore, SoftGroup can be 
      extended to perform object detection and panoptic segmentation with nontrivial improvements over existing methods.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2209.08263'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10288416'
      - name: code
        href: 'https://github.com/thangvubk/SoftGroup'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: mimo
    title:
      name: 'Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning'
      link: 'https://ieeexplore.ieee.org/document/10537203'
    authors:
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - me: true
      - name: Tri Ton
        href: 'https://triton99.github.io'
      - name: Sungwoong Kim
        href: 'https://sites.google.com/view/swkim'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Access, 2024 <br>IROS Policy Learning in Geometric Spaces Workshop, 2023 (<oral>Spotlight</oral>)'
    description: >-
      Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm, 
      which essentially requires training policies from pre-collected datasets without the need for additional 
      environment interaction. However, training under this paradigm presents its own challenge: the extrapolation 
      error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue 
      through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the 
      behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative 
      in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address 
      these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network 
      framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD 
      data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data 
      uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence 
      bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the 
      uncertainty-aware Q-function, offering the same ability for uncertainty quantification as an ensemble of networks 
      but with a cost nearly equivalent to that of a single network. Consequently, this framework strikes a harmonious 
      balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive 
      experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while 
      remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework 
      offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.
    links:
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10537203'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: dimcl
    title:
      name: 'DimCL: Dimensional Contrastive Learning for Improving Self-Supervised Learning'
      link: 'https://arxiv.org/abs/2309.11782'
    authors:
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - name: Trung X. Pham
        href: 'https://trungpx.github.io'
      - name: Chaoning Zhang
      - me: true
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Access, 2023'
    description: >-
      Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key 
      role. However, the recent development of new non-CL frameworks has achieved comparable or better performance 
      with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL 
      into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. 
      In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of 
      along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning 
      (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. 
      DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its
      success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance 
      improvement by a non-trivial margin on various datasets and backbone architectures.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2309.11782'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/10014996'
      - name: code
        href: 'https://github.com/thanhkaist/DimCL'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: softgroup
    title:
      name: 'SoftGroup for 3D Instance Segmentation on Point Clouds'
      link: 'https://arxiv.org/abs/2203.01509'
    authors:
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Kookhoi Kim
      - me: true
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022'
    description: >-
      Existing state-of-the-art 3D instance segmentation methods perform semantic segmentation followed by grouping. 
      The hard predictions are made when performing semantic segmentation such that each point is associated with a 
      single class. However, the errors stemming from hard decision propagate into grouping that results in (1) low 
      overlaps between the predicted instance with the ground truth and (2) substantial false positives. To address 
      the aforementioned problems, this paper proposes a 3D instance segmentation method referred to as SoftGroup by 
      performing bottom-up soft grouping followed by top-down refinement. SoftGroup allows each point to be associated 
      with multiple classes to mitigate the problems stemming from semantic prediction errors and suppresses false 
      positive instances by learning to categorize them as background. Experimental results on different datasets and
      multiple evaluation metrics demonstrate the efficacy of SoftGroup. Its performance surpasses the strongest prior
      method by a significant margin of +6.2% on the ScanNet v2 hidden test set and +6.8% on S3DIS Area 5 in terms of 
      AP_50. SoftGroup is also fast, running at 345ms per scan with a single Titan X on ScanNet v2 dataset.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2203.01509'
      - name: paper
        href: 'https://openaccess.thecvf.com/content/CVPR2022/papers/Vu_SoftGroup_for_3D_Instance_Segmentation_on_Point_Clouds_CVPR_2022_paper.pdf'
      - name: code
        href: 'https://github.com/thangvubk/SoftGroup'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: vpcpm
    title:
      name: 'Visual Pretraining via Contrastive Predictive Model for Pixel-Based Reinforcement Learning'
      link: 'https://www.mdpi.com/1424-8220/22/17/6504'
    authors:
      - me: true
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'MDPI Sensors, 2022'
    description: >-
      In an attempt to overcome the limitations of reward-driven representation learning in vision-based reinforcement 
      learning (RL), an unsupervised learning framework referred to as the visual pretraining via contrastive 
      predictive model (VPCPM) is proposed to learn the representations detached from the policy learning. 
      Our method enables the convolutional encoder to perceive the underlying dynamics through a pair of forward 
      and inverse models under the supervision of the contrastive loss, thus resulting in better representations. 
      In experiments with a diverse set of vision control tasks, by initializing the encoders with VPCPM, the 
      performance of state-of-the-art vision-based RL algorithms is significantly boosted, with 44% and 10% 
      improvement for RAD and DrQ at 100k steps, respectively. In comparison to the prior unsupervised methods, 
      the performance of VPCPM matches or outperforms all the baselines. We further demonstrate that the learned 
      representations successfully generalize to the new tasks that share a similar observation and action space.
    links:
      - name: paper
        href: 'https://www.mdpi.com/1424-8220/22/17/6504'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: lsf_idm
    title:
      name: 'Utilizing Skipped Frames in Action Repeats for Improving Sample Efficiency in Reinforcement Learning'
      link: 'https://ieeexplore.ieee.org/document/9793636'
    authors:
      - me: true
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Access, 2022'
    description: >-
      Action repeat has become the de-facto mechanism in deep reinforcement learning (RL) for stabilizing training 
      and enhancing exploration. Here, the action is taken at the action-decision point and is executed repeatedly 
      for a designated number of times until the next decision point. Although showing several advantages, in this 
      mechanism, the intermediate states which stem from repeated actions are discarded in training agents, causing 
      sample inefficiency. To utilize the discarded states as training data is nontrivial as the action, which causes 
      the transition between these states, is unavailable. This paper proposes to infer the action at the intermediate 
      states via an inverse dynamic model. The proposed method is simple and easily incorporated into the existing 
      off-policy RL algorithms - integrating the proposed method with SAC shows consistent improvement across various tasks.
    links:
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/9793636'
      - name: code
        href: 'https://github.com/tunglm2203/lsf_idm'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: hgr
    title:
      name: 'Hindsight Goal Ranking on Replay Buffer for Sparse Reward Environment'
      link: 'https://arxiv.org/abs/2110.15043'
    authors:
      - me: true
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE Access, 2021'
    description: >-
      This paper proposes a method for prioritizing the replay experience referred to as Hindsight Goal Ranking (HGR) 
      in overcoming the limitation of Hindsight Experience Replay (HER) that generates hindsight goals based on 
      uniform sampling. HGR samples with higher probability on the states visited in an episode with larger temporal 
      difference (TD) error, which is considered as a proxy measure of the amount which the RL agent can learn from 
      an experience. The actual sampling for large TD error is performed in two steps: first, an episode is sampled 
      from the relay buffer according to the average TD error of its experiences, and then, for the sampled episode, 
      the hindsight goal leading to larger TD error is sampled with higher probability from future visited states. 
      The proposed method combined with Deep Deterministic Policy Gradient (DDPG), an off-policy model-free actor-critic 
      algorithm, accelerates learning significantly faster than that without any prioritization on four challenging 
      simulated robotic manipulation tasks. The empirical results show that HGR uses samples more efficiently than 
      previous methods across all tasks.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2110.15043'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/9391700'
      - name: code
        href: 'https://github.com/tunglm2203/hgr'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: robust_maml
    title:
      name: 'Robust MAML: Prioritization task buffer with adaptive learning process for model-agnostic meta-learning'
      link: 'https://arxiv.org/abs/2103.08233'
    authors:
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - me: true
      - name: Trung X. Pham
        href: 'https://trungpx.github.io'
      - name: Sanzhar Rakhimkul
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021'
    description: >-
      Model agnostic meta-learning (MAML) is a popular state-of-the-art meta-learning algorithm that provides good 
      weight initialization of a model given a variety of learning tasks. The model initialized by provided weight 
      can be fine-tuned to an unseen task despite only using a small amount of samples and within a few adaptation 
      steps. MAML is simple and versatile but requires costly learning rate tuning and careful design of the task 
      distribution which affects its scalability and generalization. This paper proposes a more robust MAML based 
      on an adaptive learning scheme and a prioritization task buffer (PTB) referred to as Robust MAML (RMAML) for 
      improving scalability of training process and alleviating the problem of distribution mismatch. RMAML uses 
      gradient-based hyper-parameter optimization to automatically find the optimal learning rate and uses the PTB 
      to gradually adjust train-ing task distribution toward testing task distribution over the course of training. 
      Experimental results on meta reinforcement learning environments demonstrate a substantial performance gain
      as well as being less sensitive to hyper-parameter choice and robust to distribution mismatch.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2103.08233'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/9413446'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: sphere_rpn
    title:
      name: 'SphereRPN: Learning Spheres for High-Quality Region Proposals on 3D Point Clouds Object Detection'
      link: 'https://github.com/thangvubk/SphereRPN'
    authors:
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Kookhoi Kim
      - name: Haeyong Kang
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
      - me: true
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE International Conference on Image Processing (ICIP), 2021'
    description: >-
      A bounding box commonly serves as the proxy for 2D object detection. However, extending this practice to 3D 
      detection raises sensitivity to localization error. This problem is acute on flat objects since small localization 
      error may lead to low overlaps between the prediction and ground truth. To address this problem, this paper 
      proposes Sphere Region Proposal Network (SphereRPN) which detects objects by learning spheres as opposed to 
      bounding boxes. We demonstrate that spherical proposals are more robust to localization error compared to 
      bounding boxes. The proposed SphereRPN is not only accurate but also fast. Experiment results on the standard 
      ScanNet dataset show that the proposed SphereRPN outperforms the previous state-of-the-art methods by a large 
      margin while being 2x to 7x faster.
    links:
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/9506249'
      - name: code
        href: 'https://github.com/thangvubk/SphereRPN'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: ccfdm
    title:
      name: 'Sample-efficient reinforcement learning representation learning with curiosity contrastive forward dynamics model'
      link: https://arxiv.org/abs/2103.08255'
    authors:
      - name: Xuan Thanh Nguyen
        href: 'https://sanctusfactory.com/family_02.php#:~:text=Machine%20Learning-,Nguyen%20Xuan%20Thanh,-%E2%97%8B%20Email%20Address'
        star: true
      - me: true
        star: true
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021'
    description: >-
      Developing an agent in reinforcement learning (RL) that is capable of performing complex control tasks directly 
      from high-dimensional observation such as raw pixels is yet a challenge as efforts are made towards improving 
      sample efficiency and generalization. This paper considers a learning framework for Curiosity Contrastive 
      Forward Dynamics Model (CCFDM) in achieving a more sample-efficient RL based directly on raw pixels. 
      CCFDM incorporates a forward dynamics model (FDM) and performs contrastive learning to train its deep 
      convolutional neural network-based image encoder (IE) to extract conducive spatial and temporal information 
      for achieving a more sample efficiency for RL. In addition, during training, CCFDM provides intrinsic rewards, 
      produced based on FDM prediction error, encourages the curiosity of the RL agent to improve exploration. 
      The diverge and less-repetitive observations provide by both our exploration strategy and data augmentation 
      available in contrastive learning improve not only the sample efficiency but also the generalization. 
      Performance of existing model-free RL methods such as Soft Actor-Critic built on top of CCFDM outperforms 
      prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.
    links:
      - name: arXiv
        href: 'https://arxiv.org/abs/2103.08255'
      - name: paper
        href: 'https://ieeexplore.ieee.org/document/9636536'
      - name: code
        href: 'https://github.com/thanhkaist/CCFDM1'
    img2: false
    nogif: true
    highlighted: true
################################################################################################################
  - name: pesr
    title:
      name: 'Perception-Enhanced Image Super-Resolution via Relativistic Generative Adversarial Networks'
      link: 'https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Perception-Enhanced_Image_Super-Resolution_via_Relativistic_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf'
    authors:
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
      - me: true
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'European Conference on Computer Vision Workshop (ECCV-W), 2018'
    description: >-
      This paper considers a deep Generative Adversarial Networks (GAN) based method referred to as the 
      Perception-Enhanced Super-Resolution (PESR) for Single Image Super Resolution (SISR) that enhances the perceptual 
      quality of the reconstructed images by consid ering the following three issues: (1) ease GAN training by replacing
      an absolute with a relativistic discriminator, (2) include in the loss function a mechanism to emphasize difficult 
      training samples which are gener ally rich in texture and (3) provide a flexible quality control scheme at 
      test time to trade-off between perception and fidelity. Based on exten sive experiments on six benchmark datasets, 
      PESR outperforms recent state-of-the-art SISR methods in terms of perceptual quality.
    links:
      - name: paper
        href: 'https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Perception-Enhanced_Image_Super-Resolution_via_Relativistic_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf'
      - name: code
        href: 'https://github.com/thangvubk/PESR'
    img2: false
    nogif: true
    highlighted: false
################################################################################################################
  - name: feqe
    title: 
      name: 'Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks'
      link: 'https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf'
    authors:
      - name: Thang Vu
        href: 'https://thangvubk.github.io'
        star: false
      - name: Cao Van Nguyen
        star: false
      - name: Trung X. Pham
        href: 'https://trungpx.github.io'
        star: false
      - me: true
        star: false
      - name: Chang D. Yoo
        href: 'https://sanctusfactory.com/family.php'
        star: false
    conference:
      name: 'European Conference on Computer Vision Workshop (ECCV-W), 2018 (<oral>Winning entry, Spotlight</oral>)'
    description: >-
      This paper considers a convolutional neural network for image quality enhancement referred to as the fast and 
      efficient quality en hancement (FEQE) that can be trained for either image super-resolution or image enhancement 
      to provide accurate yet visually pleasing images on mobile devices by addressing the following three main issues. 
      First, the considered FEQE performs majority of its computation in a low resolution space. Second, the number of 
      channels used in the convolu tional layers is small which allows FEQE to be very deep. Third, the FEQE performs 
      downsampling referred to as desubpixel that does not lead to loss of information. Experimental results on a number
      of standard benchmark datasets show significant improvements in image fidelity and reduction in processing time 
      of the proposed FEQE compared to the re cent state-of-the-art methods. In the PIRM 2018 challenge, the proposed 
      FEQE placed first on the image super-resolution task for mobile devices.
    links:
      - name: paper
        href: 'https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf'
      - name: code
        href: 'https://github.com/thangvubk/FEQE'
    img2: false
    nogif: true
    highlighted: true
